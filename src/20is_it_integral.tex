\documentclass{article}
\usepackage[pagebackref,letterpaper=true,colorlinks=true,pdfpagemode=none,urlcolor=blue,linkcolor=blue,citecolor=blue,pdfstartview=FitH]{hyperref}

\usepackage{amsmath,amsfonts}
\usepackage{graphicx}
\usepackage{color}


\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}
\setlength{\textwidth}{6.0in}
\setlength{\topmargin}{0in}
\setlength{\textheight}{8.5in}


\setlength{\parindent}{0in}
\setlength{\parskip}{5px}

%\input{macrosblog}

%%%%%%%%% For wordpress conversion

\def\more{}

\newif\ifblog
\newif\iftex
\blogfalse
\textrue


\usepackage{ulem}
\def\em{\it}
\def\emph#1{\textit{#1}}

\def\image#1#2#3{\begin{center}\includegraphics[#1pt]{#3}\end{center}}

\let\hrefnosnap=\href

\newenvironment{btabular}[1]{\begin{tabular} {#1}}{\end{tabular}}

\newenvironment{red}{\color{red}}{}
\newenvironment{green}{\color{green}}{}
\newenvironment{blue}{\color{blue}}{}

%%%%%%%%% Typesetting shortcuts

\def\B{\{0,1\}}
\def\xor{\oplus}

\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\var{{\bf Var}}

\def\N{{\mathbb N}}
\def\Z{{\mathbb Z}}
\def\R{{\mathbb R}}
\def\C{{\mathbb C}}
\def\Q{{\mathbb Q}}
\def\eps{{\epsilon}}

\def\bz{{\bf z}}

\def\true{{\tt true}}
\def\false{{\tt false}}

%%%%%%%%% Theorems and proofs

\newtheorem{exercise}{Exercise}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}{Example}
\newtheorem{remark}[theorem]{Remark}
\newenvironment{proof}{\noindent {\sc Proof:}}{$\Box$} %\medskip} 
%%%%%%%%% I added
\newtheorem{assumption}{Assumption}
%%%%%%%%

\begin{document}

\section{Abstract}

\begin{itemize}
 \item We will evaluate the same definite integral
 \item We use importance sampling to improve its efficiency
 \item We use inverse transform for exact sampling
\end{itemize}

\section{Problem}

\begin{example}\label{ex:integral}
Our goal is to compute, using OMC by exact sampling
$$\alpha = \int_0^1 h(x) dx$$
where
$$h(x) = 100 \cdot I_{(0, 1/100]}(x) + 1\cdot I_{(1/100, 1)} (x).$$
\end{example}
Of course, the exact value shall be 
$$\alpha = 1.98.$$
Pretended not to know the exact value, we have used OMC with exact sampling of uniform random variable as follows:

\begin{algorithm}
\caption{Integral by MC - Example \ref{ex:integral}}
\label{alg:mcpi}
\begin{algorithmic}[1]
\Procedure{mcintegral}{$N$}\Comment{$N$ is total number of samples}
\State  $s \gets 0$ \Comment{$s$ is the sum of samples}
\For {$i = 1...N$}
	\State generate two numbers $Y$ from $U(0,1)$ \Comment{use  {\it numpy.random.uniform}}
	\State $s\gets s+h(Y)$ 
\EndFor
\State \textbf{return} $\frac{s}{N}$ \Comment{return the average}
\EndProcedure
\end{algorithmic}
\end{algorithm}

 

Next, we are going to improve the efficiency by using importance sampling. We also extend our skill on exact sampling by using inverse transform.

\section{Analysis}

\subsection{Importance sampling}
Recall that, to estimate the above integral $\alpha$, 
we use the uniform random variable $X$, 
whose density is $p(x) = I_{(0,1)}(x)$, and write
$$\alpha = \mathbb E[h(X) | X \sim p] = \int_{0}^{1} h(x) p(x) dx.$$
Naturally, one can sample iid uniform random numbers by computer, denoted by
$$\{\ X_i \sim p: i = 1, 2, \ldots, n\} ,$$
then taking their average for its approximation of $\alpha$, i.e.
$$\hat \alpha_n = \frac 1 n \sum_{i=1}^n h(X_i).$$

\begin{example}
 Compute MSE of $\hat \alpha_{n}$. Verify it with your code.
\end{example}
{\bf Solution}.
Since it is unbiased, MSE is the same as Variance of $\hat \alpha_{n}$, 
and it is again equal to $1/n$ of 
$$Var[h(X) | X\sim p].
$$Therefore, it is
$\frac{100.99}{n}$. \hfill $\Box$


IS considers, with a smart choice of a pdf $p_{1}$ (to be determined), 
$$\alpha = \int_{0}^{1} h(x)\frac{p(x)}{p_{1}(x)} p_{1} (x) dx= 
\mathbb E \Big[h(X) \frac{p(X)}{p_{1}(X)} \Big| X\sim p_{1}(x) \Big] $$
Since we observe that the interval $(0, 1/100)$ is much more {\it important}
than $(1/100, 1)$, our choice of $p_{1}$ is the following:
$$p_{1} (x) = \frac{1}{C} (2 \cdot I_{(0, 1/100]} (x) + 1 \cdot I_{(1/100, 1)}(x)),$$
where
$C = 101/100$ is the normalizing constant to make $p_{1}$ to be a valid pdf.

\begin{algorithm}\label{pc:is}
\caption{Integral by importance sampling - Example \ref{ex:integral}}
\label{alg:mcpi}
\begin{algorithmic}[1]
\Procedure{isintegral}{$N$}\Comment{$N$ is total number of samples}
\State  $s \gets 0$ \Comment{$s$ is the sum of samples}
\For {$i = 1...N$}
	\State generate a number $Y \sim p_1$ by inverse transform method \Comment{ITM to be explained}
	\State $s\gets s+ h(Y) \cdot \frac{p(Y)}{p_1(Y)}$ 
\EndFor
\State \textbf{return} $\frac{s}{N}$ \Comment{return the average}
\EndProcedure
\end{algorithmic}
\end{algorithm}

 


\begin{example}
 Prove that MSE of $\hat \alpha_{n}$ is $51.4999/n$. Verify it with your code.
\end{example}

\subsection{Inverse transform method}
To implement the IS, we  shall generate $p_{1}$ samples.
But this is not directly available by python. 
Inverse transform method provides exact sampling as long as the inverse of CDF is explicitly available. Its theoretic basis is given next.

\begin{algorithm}\label{pc:itm}
\caption{ITM sample generation for $X\sim F$ given $F^{-1}$}
\label{alg:mcpi}
\begin{algorithmic}[1]
\Procedure{isintegral}{$F^{-1}$}\Comment{$F^{-1}$ is the inverse of CDF}
\State generate a number $Y$ from $U(0,1)$ \Comment{use  {\it numpy.random.uniform}}
\State $X = F^{-1}(Y)$
\State \textbf{return} $X$ 
\EndProcedure
\end{algorithmic}
\end{algorithm}


\begin{proposition}
 Suppose $X$ has its CDF $F$ and its inverse $F^{-1}$ exists, then 
 $F^{-1}(U) \sim X$, where $U\sim U(0,1)$.
\end{proposition}
\begin{proof}
 $$\mathbb P(F^{-1}(U) \le x) = \mathbb P(U\le F(x)) = F(x).$$
\end{proof}




\end{document}
