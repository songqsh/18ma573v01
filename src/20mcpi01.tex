\documentclass{article}
\usepackage[pagebackref,letterpaper=true,colorlinks=true,pdfpagemode=none,urlcolor=blue,linkcolor=blue,citecolor=blue,pdfstartview=FitH]{hyperref}

\usepackage{amsmath,amsfonts}
\usepackage{graphicx}
\usepackage{color}


\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}
\setlength{\textwidth}{6.0in}
\setlength{\topmargin}{0in}
\setlength{\textheight}{8.5in}


\setlength{\parindent}{0in}
\setlength{\parskip}{5px}

%\input{macrosblog}

%%%%%%%%% For wordpress conversion

\def\more{}

\newif\ifblog
\newif\iftex
\blogfalse
\textrue


\usepackage{ulem}
\def\em{\it}
\def\emph#1{\textit{#1}}

\def\image#1#2#3{\begin{center}\includegraphics[#1pt]{#3}\end{center}}

\let\hrefnosnap=\href

\newenvironment{btabular}[1]{\begin{tabular} {#1}}{\end{tabular}}

\newenvironment{red}{\color{red}}{}
\newenvironment{green}{\color{green}}{}
\newenvironment{blue}{\color{blue}}{}

%%%%%%%%% Typesetting shortcuts

\def\B{\{0,1\}}
\def\xor{\oplus}

\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\var{{\bf Var}}

\def\N{{\mathbb N}}
\def\Z{{\mathbb Z}}
\def\R{{\mathbb R}}
\def\C{{\mathbb C}}
\def\Q{{\mathbb Q}}
\def\eps{{\epsilon}}

\def\bz{{\bf z}}

\def\true{{\tt true}}
\def\false{{\tt false}}

%%%%%%%%% Theorems and proofs

\newtheorem{exercise}{Exercise}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}{Example}
\newtheorem{remark}[theorem]{Remark}
\newenvironment{proof}{\noindent {\sc Proof:}}{$\Box$} %\medskip} 
%%%%%%%%% I added
\newtheorem{assumption}{Assumption}
%%%%%%%%

\begin{document}

\section{Abstract}
\begin{itemize}
 \item Using Monte Carlo to approximate $\pi$;
 \item Introduce Monte Carlo basics.
\end{itemize}

\section{Problem}
Approximate the value $\pi$.

\section{Analysis}
Consider the following question:

\begin{itemize}
 \item You shoot a square $(-1, 1)^{2}$. Suppose your shot is uniform in this square, then what is the probability you have a successful shot? We say
 ``your shot is successful'', if your shot belongs to the unit ball $B_{1}$.
\end{itemize}

The answer is 
$$\hbox{ Prob of succesful shot } = \frac{\hbox{Area of  }B_{1}} { \hbox{ Area of }(-1, 1)^{2}} = \frac{\pi}{4} .$$


This means that, as long as one can approximate probability of successful shot, one can approximate $\pi$ by multiplying $4$. This can be done by computer: 


\begin{algorithm}
\caption{MC estimation of $\pi$}\label{alg:mcpi}
\begin{algorithmic}[1]
\Procedure{mcpi}{$N$}\Comment{$N$ is total number of samples}
\State  $n \gets 0$ \Comment{$n$ is number of hits}
\For {$i = 1...N$}
	\State generate two numbers $X, Y$ from $U(-1,1)$
	\If {$X^{2} + Y^{2} <1$}
		$n \gets n+1$
	\EndIf
\EndFor
\State \textbf{return} $\frac{4n}{N}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{example}\label{exm:mcpi}
\begin{itemize}
 \item Using Algo\ref{alg:mcpi}, design estimator $\hat \pi(N)$ and 
 compute $\hat \pi(10000)$.
\end{itemize}

\end{example}


\section{Monte Carlo basics}

\subsection{Bias and MSE}
One can implement above approximation multiple times and observe that
\begin{itemize}
\item (random estimator) Target value $\pi$ is deterministic, but each implementation gives different outcome $\hat \pi$;
\item (Convergence) Each obtained outcome, as long as $N$ is large enough, gives some close approximation.
\end{itemize}

We are going to generalize our observations in this below. 
\begin{itemize}
\item
A random estimator $\hat \alpha$ to a deterministic value $\alpha$ is called as Monte Carlo (MC) approximation. 
\item Moreover, we define
$$Bias = \mathbb E [\hat \alpha] - \alpha$$
and 
$$MSE = \mathbb E [(\hat \alpha - \alpha)^2].$$
\item (def) If Bias is zero, then we call this as {\it unbiased} MC.
\end{itemize}

\begin{proposition}
 $MSE (\hat \alpha) = |Bias(\hat \alpha)|^2 + Var(\hat \alpha)$.
 In particular, if $\hat \alpha$ is unbiased, then MSE is Variance.
\end{proposition}
\begin{proof}
  ...
\end{proof}


Although seemingly absurd, we consider the above estimator with $N = 1$, which is equivalent to 
\begin{itemize}
 \item Consider 
 $$\hat \alpha = 4 I(X_1^2 + Y_1^2 <1), \ X_1, Y_1 \sim U(-1, 1)$$ 
 as MC for $\pi$. Then the outcome is either $0$ or $4$. In any case, it is a bad approximation. 
 \item However, we can show that it's an unbiased MC. (why?)
 \item Find MSE?
\end{itemize}

\subsection{Ordinary Monte Carlo}
Unbiased MC is very desirable, because one can employ crude (ordinary) MC
to make it more accurate:
\footnote{We say a random variable $X$ is in $L^p$, if its $p$th moment exists, 
i.e. $\mathbb E |X|^p <\infty .$
If $X\in L^2$, then we say it's square integrable.}
\begin{itemize}
 \item Suppose $\hat \alpha$ is a square integrable unbiased MC;
 \item Obtain $N$ independent replicates
 $$\{\hat \alpha_i: i = 1,\ldots, N\}.$$
 \item Taking their average, it gives a new MC:
 $$\beta_N = \frac 1 N \sum_{i=1}^N \hat \alpha_i.$$
 \item $\beta_N$ is unbiased again. (why?)
 \item $MSE(\beta_N) = \frac 1 N MSE(\hat \alpha) \to 0$. (why?)
 \item $\beta_N$ is {\it  almost surely consistent},
  (why?)  i.e. 
  $$\beta_{N} \to \alpha, \ \hbox{ almost surely  or }
  \mathbb P(\lim_N \beta_N = \alpha) = 1.$$
  \item $\beta_N$ is $L^2$-consistent,  (why?) i.e.
  $$\beta_{N} \to \alpha \hbox{ in } L^{2} \hbox{ or }
  \mathbb E (\beta_N - \alpha)^2 \to 0.$$
\end{itemize}



As a conclusion, one can always use crude MC to make better approximation provided there exists an unbiased MC $\hat \alpha$, which is obtainable in sacrifice of  higher computational cost.
Given a fixed amount of computational cost, to improve the efficiency, it is essential to reduce $Var(\hat \alpha)$ as much as possible.

\begin{proposition}
Prove that both almost sure and $L^2$ consistency implies consistency in probability. 
\end{proposition}

\begin{example}
 Consider $\alpha_{n}$ is a sequence of estimators to the value $\alpha$. 
 Prove that, if $MSE(\alpha_{n}) \to 0$, then $\alpha_{n}$ is $L^{2}$ consistent to $\alpha$.
\end{example}

\begin{example} \label{exm:mse}
 Given i.i.d $\{\alpha_i: i\in 1, 2, \ldots, N\}$, we use 
$$\bar \alpha_N = \frac 1 N \sum_{i=1}^N \alpha_i$$
as its estimator of the mean $\mathbb E[\alpha_1]$ 
and use
$$\beta_N = \frac 1 N \sum_{i=1}^N (\alpha_i -\bar \alpha_N)^2$$
as the estimator of $Var(\alpha_1)$. Suppose $\alpha_1\in L^4$, then 
\begin{itemize}
 \item Prove $\beta_N$ is biased.
 \item (optional) Prove that $\beta_N$ is consistent in $L^2$.
 \item Can you propose an unbiased estimator?
 \end{itemize}
\end{example}
\begin{example}
\begin{itemize}
 \item Use $\beta_{100}$ of Example \ref{exm:mse} to estimate 
 $MSE(\hat \pi_{N})$ by repeating $\pi_{N}$ of Example \ref{exm:mcpi}. One must write both pseudocode and python code.
 \item Repeat above estimation of $MSE(\hat \pi_{N})$ for $N= 2^{i}: i = 5, ...10$ and plot log-log chart.
\end{itemize}
 
\end{example}






\end{document}
